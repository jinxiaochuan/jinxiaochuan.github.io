[{"id":"1be78549da2d3dbc703ef7208cb57948","title":"Chrome 80+","content":"Chrome 80 策略更新Chrome 80 稳定版（版本号 v80.0.3987.87）已正式面向 Windows、macOS、Linux、Android 和 iOS 全平台推送\n混合内容强制 HTTPS混合内容是指 https 页面下有非 https 资源时，浏览器的加载策略。\n在 Chrome 80 中，如果你的页面开启了 https，同时你在页面中请求了 http 的音频和视频资源，这些资源将将自动升级为 https ，并且默认情况下，如果它们无法通过 https 加载，Chrome 将阻止它们。这样就会造成一些未支持 https 协议的资源加载失败。\n如果你想临时访问这些资源，你可以通过更改下面的浏览器设置来访问：\n\n\n\n\n\n\n\n\n\n\n单击地址栏上的锁定图标并选择 “站点设置”\n\n将 “隐私设置和安全性” 中的 “不安全内容” 选择为 “允许”\n\n你还可以通过设置 StricterMixedContentTreatmentEnabled 策略来控制这些变化：\n\n此策略控制浏览器中混合内容（HTTPS 站点中的 HTTP 内容）的处理方式。如果该政策设置为 true 或未设置，则音频和视频混合内容将自动升级为 HTTPS（即，URL 将被重写为 HTTPS，如果资源不能通过 HTTPS 获得，则不会进行回退），并且将显示“不安全”警告在网址列中显示图片混合内容。如果该策略设置为 false，则将禁用音频和视频的自动升级，并且不会显示图像警告。该策略不影响音频，视频和图像以外的其他类型的混合内容。\n但是以上策略是一个临时策略，将在 Chrome 84 中删除。更合理的方式是你需要推动全站资源开启 HTTPS。Chrome 也是推荐大家这么做的\n\n\n\n强推 SameSite CookieSameSite 是 Chrome 51 版本为浏览器的 Cookie 新增的了一个属性， SameSite 阻止浏览器将此 Cookie 与跨站点请求一起发送。其主要目标是降低跨源信息泄漏的风险。同时也在一定程度上阻止了 CSRF（Cross-site request forgery 跨站请求伪造）\nCookie 往往用来存储用户的身份信息，恶意网站可以设法伪造带有正确 Cookie 的 HTTP 请求，这就是 CSRF 攻击。\n\n\n\n\n\n\n\n\n\nSameSite 可以避免跨站请求发送 Cookie，有以下三个属性：\n\nStrictStrict 是最严格的防护，将阻止浏览器在所有跨站点浏览上下文中将 Cookie 发送到目标站点，即使在遵循常规链接时也是如此。因此这种设置可以阻止所有 CSRF 攻击。然而，它的用户友好性太差，即使是普通的 GET 请求它也不允许通过。例如，对于一个普通的站点，这意味着如果一个已经登录的用户跟踪一个发布在公司讨论论坛或电子邮件上的网站链接，这个站点将不会收到 Cookie ，用户访问该站点还需要重新登陆。不过，具有交易业务的网站很可能不希望从外站链接到任何交易页面，因此这种场景最适合使用 strict 标志。\nLax对于允许用户从外部链接到达本站并使用已有会话的网站站，默认的 Lax 值在安全性和可用性之间提供了合理的平衡。Lax 属性只会在使用危险 HTTP 方法发送跨域 Cookie 的时候进行阻止，例如 POST 方式。例如，一个用户在 A 站点 点击了一个 B 站点（GET 请求），而假如 B 站点 使用了 Samesite-cookies&#x3D;Lax，那么用户可以正常登录 B 站点。相对地，如果用户在 A 站点提交了一个表单到 B 站点（POST 请求），那么用户的请求将被阻止，因为浏览器不允许使用 POST 方式将 Cookie 从 A 域发送到Ｂ域。\nNone浏览器会在同站请求、跨站请求下继续发送 Cookies，不区分大小写。\n\n以上更新可能对以下功能造成影响：\n\n跨域名登陆失效\njsonp 获取数据失效\niframe 嵌套的页面打不开或异常\n部分客户端未改造导致各种数据获取异常\n\n正式支持 JavaScript Optional chaining &amp; Nullish coalescingJavaScript Optional chaining// 对象式写法\n// 通常写法\nconst nestedProp = obj.first &amp;&amp; obj.first.second;\n// 支持JavaScript Optional chaining后我们可以不用再做无谓的判断逻辑了\nconst nestedProp = obj.first?.second;\n\n// 数组式写法\n// 通常写法\nconst firstEl = arr &amp;&amp; Array.isArray(arr) &amp;&amp; arr[0];\n// 支持JavaScript Optional chaining后\nconst firstEl = arr?.[0];\n\n// 函数式写法\n// 通常写法\nconst result = someInterface.customMethod &amp;&amp; someInterface.customMethod();\n// 支持JavaScript Optional chaining后\nconst result = someInterface.customMethod?.();\n\n\n\n\n\n\n\n\n\n\n注：如果在开发的时候，需要兼容一些旧版本的浏览器的话可以考虑引入 @babel/plugin-proposal-optional-chaining 插件\nNullish coalescing/***************** Example 1 *******************/\nconst foo;\n\n//  在下面代码执行完之后foo还是undefined，并没有被赋值，bar的结果一定会是'hello'\nconst bar = foo || 'Hello!';\n\n/***************** Example 2 *******************/\n\nconst num = 0;\nconst txt = '';\n\nconst qty = num || 42;\nconst team = txt || 'AIPE-FE';\nconsole.log(qty);  // 42 and not 0\nconsole.log(team); // \"AIPE-FE\" and not ''\n\n/***************** Example 3 *******************/\n\nlet txt = '';\n\nconst team = txt || 'AIPE-FE';\nconsole.log(team); // 'AIPE-FE'\n\nlet result = txt ?? 'AIPE-FE';\nconsole.log(result); // '' (这个时候txt就不再是''了，而是被赋值成'AIPE-FE');\n\n/***************** Example 4 *******************/\nconst nullValue = null;\nconst emptyText = '';\nconst someNumber = 42;\n\nconst valA = nullValue ?? 'default for A';\nconst valB = emptyText ?? 'default for B';\nconst valC = someNumber ?? 0;\n\nconsole.log(valA); // 'default for A'\nconsole.log(valB); // '' (as the empty string is not null or undefined)\nconsole.log(valC); // 42\n\n\n\n\n\n\n\n\n\n\n注：如果在开发的时候，需要兼容一些旧版本的浏览器的话可以考虑引入 @babel/plugin-proposal-nullish-coalescing-operator 插件\nFavicon 图标支持 SVG 格式这个升级点就不用多说了\nnew CopyWebpackPlugin([\n    &#123;\n      // 我们现在可以输出svg格式的favicon了\n      from: 'images/favicon-32.svg'\n    &#125;\n])\n\n移除对 FTP 的支持\nGoogle Chrome 当前 FTP 的实现不支持加密连接（FTPS），也没有代理。 FTP 在浏览器中的使用率非常低，以致无法再投资于改进现有的 FTP 客户端。此外，所有受影响的平台上都提供了功能更强大的 FTP 客户端。 Google Chrome 72+删除了对通过 FTP 提取文档子资源和呈现顶级 FTP 资源的支持。当前，导航到 FTP URL 会导致显示目录列表或下载，具体取决于资源的类型。 Google Chrome 74+中的一个错误导致放弃了对通过 HTTP 代理访问 FTP URL 的支持。对 FTP 的代理支持已在 Google Chrome 76 中完全删除。 Google Chrome 的 FTP 实施的其余功能仅限于显示目录列表或通过未加密的连接下载资源。我们想弃用并删除此剩余功能，而不是维护不安全的 FTP 实现。\n总的来说：chrome 80+ 以后，google chrome 团队更 focus 的点就是安全相关的问题\nWeb workers 中支持 ES modulesModule Workers 是一种适用于 Web Worker 的新模式-得益于 JavaScript 模块化的优势。 Worker 构造函数现在可以接受一个{type：“ module”}选项，该选项更改了脚本的加载和执行方式，用于匹配\n&lt;script type=\"module\">\n    const worker = new Worker('worker.js', &#123;\n        type: 'module'\n    &#125;);\n&lt;/script>\n\n详见 Threading the web with module workers\nChrome 86 策略更新Chrome 86 在 2020 年 10 月推出了稳定版，现已全面应用于 Android、Chrome OS、Linux、macOS 和 Windows 等平台\n文件系统访问通过调用 showOpenFilePicker 方法，你可以唤起文件选择窗口，进而通过返回的文件句柄对文件进行读写。\nasync function getFileHandle() &#123;\n  const opts = &#123;\n    types: [\n      &#123;\n        description: 'Text Files',\n        accept: &#123;\n          'text/plain': ['.txt', '.text'],\n          'text/html': ['.html', '.htm']\n        &#125;\n      &#125;\n    ]\n  &#125;;\n  return await window.showOpenFilePicker(opts);\n&#125;\n\n全面阻止所有非 HTTPS 混合内容下载HTTPS 混合内容错误是指初始网页通过安全的 HTTPS 链接加载，但页面中其他资源，比如图像，视频，样式表，脚本却通过不安全的 HTTP 链接加载，这样就会出现混合内容错误，也就是不安全因素。\n攻击者可拦截不安全的下载地址，将程序替换成恶意软件、甚至访问更多的敏感信息。为管控这些风险，谷歌最终还是决定在 Chrome 中禁止加载不安全资源。\n从 M86 开始，图片类型的请求，会自动升级到 HTTPS，并且没有 HTTP 的降级，Audio&#x2F;Video 类型的请求早在 M80 就开始进行了自动升级\nreplaceChildren目前，要想替换某 DOM 节点下的全部子节点，必须要先通过 innerHTML 或 removeChild 删除全部子节点，然后再逐个添加，比较麻烦。为此，Chrome 支持了 replaceChildren 方法，可以用参数中的子节点列表替换原有的全部子节点，代码如下：\nparentNode.replaceChildren(newChildren);\n\n更醒目的 HTTP 安全警告在我们访问 HTTPS 网页时，地址栏最左侧会显示一个锁定图标来表明当前网站是安全的，但如果 HTTPS 网页中嵌入的是并不安全的 HTTP 表单，浏览器则不会给出任何提示信息。而实际上已经有钓鱼网站通过这种方式来盗取用户的敏感信息了。\n所以在 Chrome 86 中，如果 HTTPS 的网页中嵌入了不安全的 HTTP 表单，表单字段下方会有极为醒目的「此表单不安全」文本提示。\n\n后台标签页更省电如果一个标签页在后台运行了五分钟以上，这个页面就会被暂时冻结，相应的 CPU 使用也会被限制在 1% 左右；如果页面支持自动刷新，唤醒时间被限制在每一分钟一次。\n参考Chrome 80 版本到底升级了些什么两个你必须要重视的 Chrome 80 策略更新！！！2020 年 10 月 Chrome 86 重要更新解读Threading the web with module workers\n","slug":"chrome80","date":"2022-02-13T14:20:14.000Z","categories_index":"","tags_index":"","author_index":"Matrix"},{"id":"78e195a173181f2e8327460e98bdacbc","title":"CORS vs HSTS","content":"浏览器同源策略所谓同源就是浏览器的一个安全机制,不同源的客户端脚本没有在明确授权的情况下,不能读写对方资源。由于存在同源策略的限制,而又有需要跨域的业务,所以就有了 CORS 的出现。\n当资源位于不同协议、子域或端口的站点时，这个请求就是跨域的\n\nCORS 与 HSTSHSTS 全称：HTTP Strict Transport Security，意译：HTTP 严格传输安全，是一个 Web 安全策略机制。\n\n\n\n\n\n\n\n\n\nHSTS 解决什么问题？它解决的是：网站从 Http 转跳到 Https 时，可能出现的安全问题。Client 从 Http 切换到 Https 前是明文传输，因此是可以被 Man-In-The-Middle 劫持的，如下流程：\n\nHSTS 如何解决?要解决从 Http 切换到 Https 被劫持的问题，只要一开始就没有 Http 请求即可，流程如下：\n\nHSTS 如何知道哪些请求该转为 HTTPS，哪些不该转?\nHSTS HEADER(Strict-Transport-Security)\n最近一次请求的 HTTPS 响应中（RESPONSE）中，带上 HSTS HEADER：\nStrict-Transport-Security: &lt;max-age=>[; includeSubDomains][; preload]\n\nHSTS PRELOAD LIST\nHSTS HEADER(Strict-Transport-Security) 还是有个漏洞：\n\n如果第一次访问网站 A 就被劫持了，哪方案 1 岂不白搭？\n清 Cookies 或者 HSTS Header 过期了，下次访问岂不又风险重重？基于以上问题，就需要方案 2（HSTS Preload List）：\n\n官方说明：\nThis is a list of sites that are hardcoded into Chrome as being HTTPS only.\nHSTS Preload List 是一个站点列表，它被 hardcode 写入 Chrome 中，列表中的站点将会默认使用 HTTPS 进行访问。\n\nMost major browsers (Chrome, Firefox, Opera, Safari, IE 11 and Edge) also have HSTS preload lists based on the Chrome list. (See the HSTS compatibility matrix.)\n主流浏览器（Firefox, Opera, Safari, IE 11 and Edge）都有和 Chrome 一样的 HSTS Preload List。\nHSTS 可能导致 CORS 产生跨域问题HSTS (HTTP Strict Transport Security) 为了避免 HTTP 跳转到 HTTPS 时遭受潜在的中间人攻击，由浏览器本身控制到 HTTPS 的跳转。如同 CORS 一样，它也是有一个服务器的响应头 Strict-Transport-Security: max-age=5184000 来控制，此时浏览器访问该域名时，会使用 307 Internal Redirect，无需服务器干涉，自动跳转到 HTTPS 请求。\n「如果前端访问 HTTP 跨域请求，此时浏览器通过 HSTS 跳转到 HTTPS，但浏览器不会给出相应的 CORS 响应头部，就会发生跨域问题。」\nGET / HTTP/1.1\nHost: shanyue.tech\nOrigin: http://shanyue.tech\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\nAccess to XMLHttpRequest at 'xxx' from origin 'xxx' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.\n\nCORSCORS 即跨域资源共享 (Cross-Origin Resource Sharing, CORS)。简而言之，就是在服务器端的响应中加入几个标头，使得浏览器能够跨域访问资源。\n这个响应头的字段设置就是：Access-Control-Allow-Origin: *\n浏览器将 CORS 请求分成两类：简单请求和非简单请求\n\n\n\n\n\n\n\n\n\n简单请求Method: 请求的方法是 GET、POST 及 HEADHeader: 请求头是 Content-Type (有限制)、Accept-Language、Content-Language 等Content-Type: 请求类型是 application&#x2F;x-www-form-urlencoded、multipart&#x2F;form-data 或 text&#x2F;plain\n非简单请求除简单请求外，均为非简单请求。一般需要开发者主动构造，在项目中常见的 Content-Type: application&#x2F;json 及 Authorization:  为典型的「非简单请求」\n预检请求(preflight request)非简单请求的 CORS 请求是会在正式通信之前进行一次预检请求，”预检”使用的请求方法是 OPTIONS , 表示这个请求是用来询问的\n// 跨域请求\nvar url = 'http://localhost:2333/cors';\nvar xhr = new XMLHttpRequest();\nxhr.open('PUT', url, true);\nxhr.setRequestHeader('X-Custom-Header', 'value');\nxhr.send();\n\n由于上面的代码使用的是 PUT 方法,并且发送了一个自定义头信息.所以是一个非简单请求,当浏览器发现这是一个非简单请求的时候,会自动发出预检请求,看看服务器可不可以接收这种请求,下面是”预检”的 HTTP 头信息\n// 跨域请求 预检 request headers\nOPTIONS /cors HTTP/1.1\nOrigin: localhost:2333\nAccess-Control-Request-Method: PUT // 表示使用的什么HTTP请求方法\nAccess-Control-Request-Headers: X-Custom-Header // 表示浏览器发送的自定义字段\nHost: localhost:2332\nAccept-Language: zh-CN,zh;q=0.9\nConnection: keep-alive\nUser-Agent: Mozilla/5.0...\n\n预检请求后的回应，服务器收到”预检”请求以后，检查了 Origin、Access-Control-Request-Method 和 Access-Control-Request-Headers 字段以后，确认允许跨源请求，就可以做出回应。\n// 跨域请求 预检 response headers\nHTTP/1.1 200 OK\nDate: Mon, 01 Dec 2008 01:15:39 GMT\nServer: Apache/2.0.61 (Unix)\nAccess-Control-Allow-Origin: http://localhost:2332 // 表示http://localhost:2332可以访问数据\nAccess-Control-Allow-Methods: GET, POST, PUT\nAccess-Control-Allow-Headers: X-Custom-Header\nContent-Type: text/html; charset=utf-8\nContent-Encoding: gzip\nContent-Length: 0\nKeep-Alive: timeout=2, max=100\nConnection: Keep-Alive\nContent-Type: text/plain\n\nCORS Response HeadersAccess-Control-Allow-Origin: 可以把资源共享给那些域名，支持 * 及 特定域名Access-Control-Allow-Credentials: 请求是否可以带 cookieAccess-Control-Allow-Methods: 请求所允许的方法, 「用于预检请求中」Access-Control-Allow-Headers: 请求所允许的头，「用于预检请求中」Access-Control-Expose-Headers: 那些头可以在响应中列出Access-Control-Max-Age: 预检请求的缓存时间\nKoa CORS 中间件原理\n必须校验是否有 Origin 请求头，跨域请求一定会有 Origin 请求头，@koa/cors 中间件源码 origin = options.origin || requestOrigin 可知，优先取 options.origin，默认以当前请求的 Origin 请求头作为 Access-Control-Allow-Origin 的响应头信息，保证当前请求允许跨域。\nmodule.exports = function(options) &#123;\n const defaults = &#123;\n   allowMethods: 'GET,HEAD,PUT,POST,DELETE,PATCH',\n &#125;;\n\n options = &#123;\n   ...defaults,\n   ...options,\n &#125;;\n\n // ...\n\n return async function cors(ctx, next) &#123;\n   // If the Origin header is not present terminate this set of steps.\n   // The request is outside the scope of this specification.\n   const requestOrigin = ctx.get('Origin');\n\n   // Always set Vary header\n   // https://github.com/rs/cors/issues/10\n   ctx.vary('Origin');\n\n   if (!requestOrigin) return await next();\n\n   let origin;\n   if (typeof options.origin === 'function') &#123;\n     origin = options.origin(ctx);\n     if (origin instanceof Promise) origin = await origin;\n     if (!origin) return await next();\n   &#125; else &#123;\n     origin = options.origin || requestOrigin;\n   &#125;\n\n   let credentials;\n   if (typeof options.credentials === 'function') &#123;\n     credentials = options.credentials(ctx);\n     if (credentials instanceof Promise) credentials = await credentials;\n   &#125; else &#123;\n     credentials = !!options.credentials;\n   &#125;\n\n   const headersSet = &#123;&#125;;\n\n   function set(key, value) &#123;\n     ctx.set(key, value);\n     headersSet[key] = value;\n   &#125;\n\n   if (ctx.method !== 'OPTIONS') &#123;\n     // Simple Cross-Origin Request, Actual Request, and Redirects\n     set('Access-Control-Allow-Origin', origin);\n\n     if (credentials === true) &#123;\n       set('Access-Control-Allow-Credentials', 'true');\n     &#125;\n\n     if (options.exposeHeaders) &#123;\n       set('Access-Control-Expose-Headers', options.exposeHeaders);\n    &#125;\n\n     if (!options.keepHeadersOnError) &#123;\n       return await next();\n     &#125;\n     try &#123;\n       return await next();\n     &#125; catch (err) &#123;\n       const errHeadersSet = err.headers || &#123;&#125;;\n       const varyWithOrigin = vary.append(errHeadersSet.vary || errHeadersSet.Vary || '', 'Origin');\n       delete errHeadersSet.Vary;\n\n       err.headers = &#123;\n         ...errHeadersSet,\n         ...headersSet,\n         ...&#123; vary: varyWithOrigin &#125;,\n       &#125;;\n       throw err;\n     &#125;\n   &#125; else &#123;\n     // Preflight Request\n\n     // If there is no Access-Control-Request-Method header or if parsing failed,\n     // do not set any additional headers and terminate this set of steps.\n     // The request is outside the scope of this specification.\n     if (!ctx.get('Access-Control-Request-Method')) &#123;\n       // this not preflight request, ignore it\n       return await next();\n     &#125;\n\n     ctx.set('Access-Control-Allow-Origin', origin);\n\n     if (credentials === true) &#123;\n       ctx.set('Access-Control-Allow-Credentials', 'true');\n     &#125;\n\n     if (options.maxAge) &#123;\n       ctx.set('Access-Control-Max-Age', options.maxAge);\n     &#125;\n\n     if (options.allowMethods) &#123;\n       ctx.set('Access-Control-Allow-Methods', options.allowMethods);\n     &#125;\n\n     let allowHeaders = options.allowHeaders;\n     if (!allowHeaders) &#123;\n       allowHeaders = ctx.get('Access-Control-Request-Headers');\n     &#125;\n     if (allowHeaders) &#123;\n       ctx.set('Access-Control-Allow-Headers', allowHeaders);\n     &#125;\n\n     ctx.status = 204;\n   &#125;\n &#125;;\n&#125;;\n\nCORS 与 Vary: Origin当请求网络静态资源为跨域资源（常见的为 cdn 静态资源，如 js、css、images 等）时，通常是会设置相关协商缓存响应头（Last-Modified、Etag）。这种一旦相同的资源链接需要根据不同的请求头（如User-Agent）响应不同的静态资源（PC、Mobile）时，可能会导致缓存资源响应错乱，此时必须设置 Vary: Origin, User-Agent，即代表为不同的 Origin 或 User-Agent 缓存不同的资源。\n详见 浏览器缓存及内容协商\n参考浏览器中的跨域问题与 CORS面试官问我 CORS 跨域，我直接一套操作斩杀！浅析 HSTS简单易懂 HSTS，你需要它！【安全】HSTS - 强制客户端（如浏览器）使用 HTTPS 与服务器创建连接\n","slug":"cors","date":"2022-02-12T16:54:51.000Z","categories_index":"","tags_index":"HTTP","author_index":"Matrix"},{"id":"0f1e554315f0274ca298957330a8910f","title":"浏览器缓存及内容协商","content":"浏览器的缓存机制浏览器的缓存机制也就是我们说的 HTTP 缓存机制，其机制是根据 HTTP 报文的缓存标识进行的。\n\n\n\n\n\n\n\n\n\n浏览器缓存过程： 强缓存、协商缓存。浏览器缓存位置一般分为四类： Service Worker、Memory Cache、Disk Cache、Push Cache\n强缓存强缓存是当我们访问 URL 的时候，不会向服务器发送请求，直接从缓存中读取资源，但是会返回 200 的状态码。\n如何设置强缓存？我们第一次进入页面，请求服务器，然后服务器进行应答，浏览器会根据 response Header 来判断是否对资源进行缓存，如果响应头中 expires、pragma 或者 cache-control 字段，代表这是强缓存，浏览器就会把资源缓存在 memory cache 或 disk cache 中。\n第二次请求时，浏览器判断请求参数，如果符合强缓存条件就直接返回状态码 200，从本地缓存中拿数据。否则把响应参数存在 request header 请求头中，看是否符合协商缓存，符合则返回状态码 304，不符合则服务器会返回全新资源。\n\n\n\n\n\n\n\n\n\n\nExpires是 HTTP1.0 控制网页缓存的字段，值为一个时间戳，准确来讲是格林尼治时间，服务器返回该请求结果缓存的到期时间，意思是，再次发送请求时，如果未超过过期时间，直接使用该缓存，如果过期了则重新请求。缺点：就是它判断是否过期是用本地时间来判断的，本地时间是可以自己修改的。\nCache-Control是 HTTP1.1 中控制网页缓存的字段，当 Cache-Control 都存在时，Cache-Control 优先级更高\n\npublic：资源客户端和服务器都可以缓存。\nprivate：资源只有客户端可以缓存。\nno-cache：客户端缓存资源，但是是否缓存需要经过协商缓存来验证。\nno-store：不使用缓存。\nmax-age：最大过期时间（距离当前请求的时间差）\n\n\nCache-Control 使用了 max-age 相对时间，解决了 expires 的问题\nPragma这个是 HTTP1.0 中禁用网页缓存的字段，其取值为 no-cache，和 Cache-Control 的 no-cache 效果一样\nVary(内容协商)要了解 Vary 的作用，先得了解 HTTP 的内容协商机制。有时候，同一个 URL 可以提供多份不同的文档，这就要求服务端和客户端之间有一个选择最合适版本的机制，这就是内容协商.服务端根据客户端发送的请求头中某些字段自动发送最合适的版本。可以用于这个机制的请求头字段又分两种：内容协商专用字段（Accept 字段）、其他字段.\n\n\n\n请求头字段\n说明\n响应头字段\n\n\n\nAccept\n告知服务器发送何种媒体类型\nContent-Type\n\n\nAccept-Language\n告知服务器发送何种语言\nContent-Language\n\n\nAccept-Charset\n告知服务器发送何种字符集\nContent-Type\n\n\nAccept-Encoding\n告知服务器采用何种压缩方式\nContent-Encoding\n\n\n// 例如客户端发送以下请求头\n// 表示它可以接受任何 MIME 类型的资源；支持采用 gzip、deflate 或 sdch 压缩过的资源；可以接受 zh-CN、en-US 和 en 三种语言，并且 zh-CN 的权重最高（q 取值 0 - 1，最高为 1，最低为 0，默认为 1），服务端应该优先返回语言等于 zh-CN 的版本。\nAccept:*/*\nAccept-Encoding:gzip,deflate,sdch\nAccept-Language:zh-CN,en-US;q=0.8,en;q=0.6\n\n// 浏览器的响应头可能是这样的\n// 表示这个文档确切的 MIME 类型是 text/javascript；文档内容进行了 gzip 压缩；响应头没有 Content-Language 字段，通常说明返回版本的语言正好是请求头 Accept-Language 中权重最高的那个\nContent-Type: text/javascript\nContent-Encoding: gzip\n\n上面四个 Accept 字段并不够用，例如要针对特定浏览器如 IE6 输出不一样的内容，就需要用到请求头中的 User-Agent 字段。类似的，请求头中的 Cookie 也可能被服务端用做输出差异化内容的依据。由于客户端和服务端之间可能存在一个或多个中间实体（如缓存服务器），而缓存服务最基本的要求是给用户返回正确的文档。如果服务端根据不同 User-Agent 返回不同内容，而缓存服务器把 IE6 用户的响应缓存下来，并返回给使用其他浏览器的用户，肯定会出问题所以 HTTP 协议规定，如果服务端提供的内容取决于 User-Agent 这样「常规 Accept 协商字段之外」的请求头字段，那么响应头中必须包含 Vary 字段，且 Vary 的内容必须包含 User-Agent。同理，如果服务端同时使用请求头中 User-Agent 和 Cookie 这两个字段来生成内容，那么响应中的 Vary 字段看上去应该是这样的：\nVary: User-Agent, Cookie\n\n也就是说 Vary 字段用于列出一个响应字段列表，告诉缓存服务器遇到同一个 URL 对应着不同版本文档的情况时，如何缓存和筛选合适的版本。\n\n\n缓存位置强缓存会把资源房放到 memory cache 和 disk cache 中，那什么资源放在 memory cache，什么资源放在 disk cache 中？\n\n查找浏览器缓存时会按顺序查找: Service Worker -&gt; Memory Cache -&gt; Disk Cache -&gt; Push Cache。\n\n\n\n\n\n\n\n\n\nService Worker是运行在浏览器背后的独立线程，一般可以用来实现缓存功能。使用 Service Worker 的话，传输协议必须为 HTTPS。因为 Service Worker 中涉及到请求拦截，所以必须使用 HTTPS 协议来保障安全。Service Worker 的缓存与浏览器其他内建的缓存机制不同，它可以让我们自由控制缓存哪些文件、如何匹配缓存、如何读取缓存，并且缓存是持续性的。\nMemory Cache内存中的缓存，主要包含的是当前中页面中已经抓取到的资源，例如页面上已经下载的样式、脚本、图片等。读取内存中的数据肯定比磁盘快，内存缓存虽然读取高效，可是缓存持续性很短，会随着进程的释放而释放。一旦我们关闭 Tab 页面，内存中的缓存也就被释放了。\nDisk Cache存储在硬盘中的缓存，读取速度慢点，但是什么都能存储到磁盘中，比之 Memory Cache 胜在容量和存储时效性上。在所有浏览器缓存中，Disk Cache 覆盖面基本是最大的。它会根据 HTTP Herder 中的字段判断哪些资源需要缓存，哪些资源可以不请求直接使用，哪些资源已经过期需要重新请求。并且即使在跨站点的情况下，相同地址的资源一旦被硬盘缓存下来，就不会再次去请求数据。绝大部分的缓存都来自 Disk Cache。memory cache 要比 disk cache 快的多。举个例子：从远程 web 服务器直接提取访问文件可能需要 500 毫秒(半秒)，那么磁盘访问可能需要 10-20 毫秒，而内存访问只需要 100 纳秒，更高级的还有 L1 缓存访问(最快和最小的 CPU 缓存)只需要 0.5 纳秒。很神奇的，我们又看到了一个 prefetch cache，这个又是什么呢?\n\nprefetch cache(预取缓存)link 标签上带了 prefetch，再次加载会出现。prefetch 是预加载的一种方式，被标记为 prefetch 的资源，将会被浏览器在空闲时间加载。 4. Push Cache\nPush Cache(推送缓存)是 HTTP&#x2F;2 中的内容，当以上三种缓存都没有命中时，它才会被使用。它只在会话（Session）中存在，一旦会话结束就被释放，并且缓存时间也很短暂，在 Chrome 浏览器中只有 5 分钟左右，同时它也并非严格执行 HTTP 头中的缓存指令。\n\n协商缓存协商缓存就是强缓存失效后，浏览器携带缓存标识向服务器发送请求，由服务器根据缓存标识来决定是否使用缓存的过程。\n协商缓存生效，返回 304\n协商缓存失效，返回 200 和请求结果\n\n\n\n\n\n\n\n\n\nLast-Modified &#x2F; If-Modified-SinceLast-Modified 是服务器响应请求时，返回该资源文件在服务器最后被修改的时间。If-Modified-Since 则是客户端再次发起该请求时，携带上次请求返回的 Last-Modified 值，通过此字段值告诉服务器该资源上次请求返回的最后被修改时间。服务器收到该请求，发现请求头含有 If-Modified-Since 字段，则会根据 If-Modified-Since 的字段值与该资源在服务器的最后被修改时间做对比，若服务器的资源最后被修改时间大于 If-Modified-Since 的字段值，则重新返回资源，状态码为 200；否则则返回 304，代表资源无更新，可继续使用缓存文件。\nEtag &#x2F; If-None-MatchEtag 是服务器响应请求时，返回当前资源文件的一个唯一标识(由服务器生成)。If-None-Match 是客户端再次发起该请求时，携带上次请求返回的唯一标识 Etag 值，通过此字段值告诉服务器该资源上次请求返回的唯一标识值。服务器收到该请求后，发现该请求头中含有 If-None-Match，则会根据 If-None-Match 的字段值与该资源在服务器的 Etag 值做对比，一致则返回 304，代表资源无更新，继续使用缓存文件；不一致则重新返回资源文件，状态码为 200。\nCORS 与 Vary: Origin在讨论 CORS 与 Vary 关系时，先抛出一个问题：\n\n\n\n\n\n\n\n\n\n如何避免 CDN 为 PC 端缓存移动端页面？\n假设有两个域名访问 static.shanyue.tech 的跨域资源\n\nfoo.shanyue.tech，响应头中返回 Access-Control-Allow-Origin: foo.shanyue.tech\nbar.shanyue.tech，响应头中返回 Access-Control-Allow-Origin: bar.shanyue.tech\n\n看起来一切正常，但平静的水面下波涛暗涌:\n「如果 static.shanyue.tech 资源被 CDN 缓存，bar.shanyue.tech 再次访问资源时，因缓存问题，因此此时返回的是 Access-Control-Allow-Origin: foo.shanyue.tech，此时会有跨域问题」\n此时，Vary: Origin 就上场了，代表为不同的 Origin 缓存不同的资源，这在各个服务器端 CORS 中间件也能体现出来。\nKoa 关于 CORS 的处理函数\nreturn async function cors(ctx, next) &#123;\n  // If the Origin header is not present terminate this set of steps.\n  // The request is outside the scope of this specification.\n  const requestOrigin = ctx.get('Origin');\n\n  // Always set Vary header\n  // https://github.com/rs/cors/issues/10\n  ctx.vary('Origin');\n&#125;\n\n\n\n\n\n\n\n\n\n\n服务器端通过响应头 Origin 来判断是否为跨域请求，并以此设置多域名跨域，但要加上 Vary: Origin。\n参考彻底理解浏览器的缓存机制前端浏览器缓存知识梳理(1.6w 字)浏览器灵魂之问，请问你能接得住几个？HTTP 协议中 Vary 的一些研究浏览器中的跨域问题与 CORS\n","slug":"broswer-cache","date":"2022-02-12T13:40:50.000Z","categories_index":"","tags_index":"HTTP","author_index":"Matrix"},{"id":"230f463a6828a545fa6d14af02ab1fd6","title":"Monorepo","content":"什么是 Monorepo?Monorepo 其实不是一个新的概念，在软件工程领域，它已经有着十多年的历史了。概念上很好理解，就是把多个项目放在一个仓库里面，相对立的是传统的 MultiRepo 模式，即每个项目对应一个单独的仓库来分散管理。\nMonorepo 是一种将多个项目代码存储在一个仓库里的软件开发策略（”mono” 来源于希腊语 μόνος 意味单个的，而 “repo”，显而易见地，是 repository 的缩写）。\n\n现代的前端工程已经越来越离不开 Monorepo 了，无论是业务代码还是工具库，越来越多的项目已经采用 Monorepo 的方式来进行开发。Google 宁愿把所有的代码都放在一个 Monorepo 工程下面，Vue 3、Yarn、Npm7 等等知名开源项目的源码也是采用 Monorepo 的方式来进行管理的。\n一般 Monorepo 的目录如下所示，在 packages 存放多个子项目，并且每个子项目都有自己的 package.json:\n├── packages\n|   ├── pkg1\n|   |   ├── package.json\n|   ├── pkg2\n|   |   ├── package.json\n├── package.json\n\n\n那 Monorepo 究竟有什么魔力，让大家如此推崇，落地如此之广呢？\nMonorepo 最佳实践之 Yarn WorkspacesYarn Workspaces（工作空间&#x2F;工作区，本文使用工作空间这一名称）是 Yarn 提供的 Monorepo 依赖管理机制，从 Yarn 1.0 开始默认支持，用于在代码仓库的根目录下管理多个 project 的依赖。\nYarn Workspaces 的目标是令使用 Monorepo 变得简单，以一种更具声明性的方式处理 yarn link 的主要使用场景。简而言之，它们允许多个项目共存在同一个代码库中，并相互交叉引用，并且保证一个项目源代码的任何修改都会立即应用到其他项目中。\n重复安装、管理繁琐的缺点从 npm package 诞生起便一直存在，node_modules hell 就是该问题的集中体现。\n\n为了简化流程，很多大型项目采用了 Monorepo 的做法，即把所有的包放在一个仓库中管理\n\n\n\n\n\n\n\n\n\nBabel、React、Vue、Jest 等都使用了 monorepo 的管理方式。\nMenorepo 的优点是可以在一个仓库里维护多个 package，可统一构建，跨 package 调试、依赖管理、版本发布都十分方便，搭配工具还能统一生成 CHANGELOG；代价是即使只开发其中一个 package 也需要安装整个项目的依赖。以 jest 为例，其 Monorepo 代码结构为：\n| jest/\n| ---- package.json\n| ---- packages/\n| -------- babel-jest/\n| ------------ package.json\n| -------- babel-plugin-jest-hoist/\n| ------------ package.json\n| -------- babel-preset-jest/\n| ------------ package.json\n| -------- .../\n\n\n为何使用 Yarn Workspaces在以 Monorepo 为代码组织方式的项目中，依赖管理的规模和复杂度均有不小的提升（这也不难理解，随着”数量“的增加，任何小的问题都会变得复杂）。如何减少依赖重复安装？如何优雅实现跨目录代码共享？如何对依赖版本进行统一管理以避免版本冲突？所以这些问题都可以借助 Yarn Workspaces 来解决！Yarn 官方对于 Yarn Workspaces 的使用时机（Why would you want to do this?）是这样描述的：\n\n\n\n\n\n\n\n\n\nYour dependencies can be linked together, which means that your workspaces can depend on one another while always using the most up-to-date code available. This is also a better mechanism than yarn link since it only affects your workspace tree rather than your whole system.\n工作区内的依赖关系可以链接在一起，这意味着工作区可以相互依赖，同时始终使用最新的可用代码。这也是一个相对于 yarn link 更好的机制，因为它只影响你的工作空间树，而不是整个系统。\nAll your project dependencies will be installed together, giving Yarn more latitude to better optimize them.\n所有的项目依赖关系都将被安装在一起，为 Yarn 提供更多的自由度来更好地优化它们。\nYarn will use a single lockfile rather than a different one for each project, which means fewer conflicts and easier reviews.\n对于每个项目，Yarn 将使用一个公共的的锁文件而不是为每个工程使用一个不同的锁文件，这意味着更少的冲突和更容易的版本审查。\n如何启用 Workspace\n\n\n\n\n\n\n\n\n\n确保项目中安装有 yarn\n在项目根目录的 packag.json 中增加如下配置:\n\n&#123;\n  \"private\": true,\n  \"workspaces\": [\"packages/*\"]\n&#125;\nmonorepo 方案实践锁定环境：VoltaVolta 是一个 JavaScript 工具管理器，它可以让我们轻松地在项目中锁定 node，npm 和 yarn 的版本。你只需在安装完 Volta 后，在项目的根目录中执行 volta pin 命令，那么无论您当前使用的 node 或 npm（yarn）版本是什么，volta 都会自动切换为您指定的版本。\n相较于 nvm，Volta 还具有一个诱人的特性：当您项目的 CLI 工具与全局 CLI 工具不一致时，Volta 可以做到在项目根目录下自动识别，切换到项目指定的版本\n复用 packages：workspace使用 monorepo 策略后，收益最大的两点是：\n避免重复安装包，因此减少了磁盘空间的占用，并降低了构建时间；内部代码可以彼此相互引用；这两项好处全部都可以由一个成熟的包管理工具来完成，对前端开发而言，即是 yarn（1.0 以上）或 npm（7.0 以上）通过名为 workspaces 的特性实现的（⚠️ 注意，支持 workspaces 特性的 npm 目前依旧不是 LTS 版本）。\n为了实现前面提到的两点收益，您需要在代码中做三件事：\n\n调整目录结构，将相互关联的项目放置在同一个目录，推荐命名为 packages；\n在项目根目录里的 package.json 文件中，设置 workspaces 属性，属性值为之前创建的目录；\n同样，在 package.json 文件中，设置 private 属性为 true（为了避免我们误操作将仓库发布）；\n\n经过修改，您的项目目录看起来应该是这样：\n.\n├── package.json\n└── packages/\n    ├── @mono/project_1/ # 推荐使用 `@&lt;项目名>/&lt;子项目名>` 的方式命名\n    │   ├── index.js\n    │   └── package.json\n    └── @mono/project_2/\n        ├── index.js\n        └── package.json\n\n\n而当您在项目根目录中执行 npm install 或 yarn install 后，您会发现在项目根目录中出现了 node_modules 目录，并且该目录不仅拥有所有子项目共用的 npm 包，还包含了我们的子项目。因此，我们可以在子项目中通过各种模块引入机制，像引入一般的 npm 模块一样引入其他子项目的代码。请注意我们对子项目的命名，统一以 @&#x2F; 开头，这是一种社区最佳实践，不仅可以让用户更容易了解整个应用的架构，也方便您在项目中更快捷的找到所需的子项目。至此，我们已经完成了 monorepo 策略的核心部分，实在是很容易不是吗？但是老话说「行百里者半九十」，距离优雅的搭建一个 monorepo 项目，我们还有一些路要走。\n统一配置：合并同类项 - Eslint，Typescript 与 BabelTypeScript我们可以在 packages 目录中放置 tsconfig.settting.json 文件，并在文件中定义通用的 ts 配置，然后，在每个子项目中，我们可以通过 extends 属性，引入通用配置，并设置 compilerOptions.composite 的值为 true，理想情况下，子项目中的 tsconfig 文件应该仅包含下述内容：\n&#123;\n  \"extends\": \"../tsconfig.setting.json\", // 继承 packages 目录下通用配置\n  \"compilerOptions\": &#123;\n    \"composite\": true, // 用于帮助 TypeScript 快速确定引用工程的输出文件位置\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\"\n  &#125;,\n  \"include\": [\"src\"]\n&#125;\n\n\nEslint对于 Eslint 配置文件，我们也可以如法炮制，这样定义子项目的 .eslintrc 文件内容：\n&#123;\n  \"extends\": \"../../.eslintrc\", // 注意这里的不同\n  \"parserOptions\": &#123;\n    \"project\": \"tsconfig.json\"\n  &#125;\n&#125;\n\nBabelBabel 配置文件合并的方式与 TypeScript 如出一辙，甚至更加简单，我们只需在子项目中的 .babelrc 文件中这样声明即可：\n&#123;\n  \"extends\": \"../.babelrc\"\n&#125;\n\n当一切准备就绪后，我们的项目目录应该大致呈如下所示的结构：\n.\n├── package.json\n├── .eslintrc\n└── packages/\n    │   ├── tsconfig.settings.json\n    │   ├── .babelrc\n    ├── @mono/project_1/\n    │   ├── index.js\n    │   ├── .eslintrc\n    │   ├── .babelrc\n    │   ├── tsconfig.json\n    │   └── package.json\n    └───@mono/project_2/\n        ├── index.js\n        ├── .eslintrc\n        ├── .babelrc\n        ├── tsconfig.json\n        └── package.json\n\n\n统一命令脚本：scripty如果您的子项目足够多，您可能会发现，每个 package.json 文件中的 scripts 属性都大同小异，并且一些 scripts 充斥着各种 Linux 语法，例如管道操作符，重定向或目录生成。重复带来低效，复杂则使人难以理解，这都是需要我们解决的问题。\n这里给出的解决方案是，使用 scripty 管理您的脚本命令，简单来说，scripty 允许您将脚本命令定义在文件中，并在 package.json 文件中直接通过文件名来引用。这使我们可以实现如下目的：\n子项目间复用脚本命令；像写代码一样编写脚本命令，无论它有多复杂，而在调用时，像调用函数一样调用；\n通过使用 scripty 管理我们的 monorepo 应用，目录结构看起来将会是这样：\n.\n├── package.json\n├── .eslintrc\n├── scirpts/ # 这里存放所有的脚本\n│   │   ├── packages/ # 包级别脚本\n│   │   │   ├── build.sh\n│   │   │   └── test.sh\n│   └───└── workspaces/ # 全局脚本\n│           ├── build.sh\n│           └── test.sh\n└── packages/\n    │   ├── tsconfig.settings.json\n    │   ├── .babelrc\n    ├── @mono/project_1/\n    │   ├── index.js\n    │   ├── .eslintrc\n    │   ├── .babelrc\n    │   ├── tsconfig.json\n    │   └── package.json\n    └── @mono/project_2/\n        ├── index.js\n        ├── .eslintrc\n        ├── .babelrc\n        ├── tsconfig.json\n        └── package.json\n\n注意，我们脚本分为两类「package 级别」与「workspace 级别」，并且分别放在两个文件夹内。这样做的好处在于，我们既可以在项目根目录执行全局脚本，也可以针对单个项目执行特定的脚本。\n通过使用 scripty，子项目的 package.json 文件中的 scripts 属性将变得非常精简：\n&#123;\n  ...\n  \"scripts\": &#123;\n    \"test\": \"scripty\",\n    \"lint\": \"scripty\",\n    \"build\": \"scripty\"\n  &#125;,\n  \"scripty\": &#123;\n    \"path\": \"../../scripts/packages\" // 注意这里我们指定了 scripty 的路径\n  &#125;,\n  ...\n&#125;\n\n统一包管理：Lerna\n\n\n\n\n\n\n\n\n\n当多个子项目放在一个代码仓库，并且子项目之间又相互依赖时，我们面临的棘手问题有两个：\n\n如果我们需要在多个子目录执行相同的命令，我们需要手动进入各个目录，并执行命令；\n\n当一个子项目更新后，我们只能手动追踪依赖该项目的其他子项目，并升级其版本。\n\n\n通过使用 Lerna，这些棘手的问题都将不复存在。当在项目根目录使用 npx lerna init 初始化后，我们的根目录会新增一个 lerna.json 文件，默认内容为：\n&#123;\n  \"packages\": [\"packages/*\"],\n  \"version\": \"0.0.0\"\n&#125;\n\n让我们稍稍改动这个文件，使其变为：\n&#123;\n  \"packages\": [\"packages/*\"],\n  \"npmClient\": \"yarn\",\n  \"version\": \"independent\",\n  \"useWorkspaces\": true,\n&#125;\n\n可以注意到，我们显示声明了我们的包客户端（npmClient）为 yarn，并且让 Lerna 追踪我们 workspaces 设置的目录，这样我们就依旧保留了之前 workspaces 的所有特性（子项目引用和通用包提升）。\n除此之外一个有趣的改动在于我们将 version 属性指定为一个关键字 independent，这将告诉 lerna 应该将每个子项目的版本号看作是相互独立的。当某个子项目代码更新后，运行 lerna publish 时，Lerna 将监听到代码变化的子项目并以交互式 CLI 方式让开发者决定需要升级的版本号，关联的子项目版本号不会自动升级，反之，当我们填入固定的版本号时，则任一子项目的代码变动，都会导致所有子项目的版本号基于当前指定的版本号升级。\nLerna 提供了很多 CLI 命令以满足我们的各种需求，但根据 2&#x2F;8 法则，您应该首先关注以下这些命令：\n\nlerna init：常见一个新的 lerna 仓库（repo）或将现有的仓库升级为适配当前 版本的 Lerna。参数 --independent/-i – 使用独立的版本控制模式\nlerna bootstrap：等同于 lerna link + yarn install，用于创建符合链接并安装依赖包；\nlerna run：会像执行一个 for 循环一样，在所有子项目中执行 npm script 脚本，并且，它会非常智能的识别依赖关系，并从根依赖开始执行命令；\nlerna exec：像 lerna run 一样，会按照依赖顺序执行命令，不同的是，它可以执行任何命令，例如 shell 脚本；\nlerna publish：发布代码有变动的 package，因此首先您需要在使用 Lerna 前使用 git commit 命令提交代码，好让 Lerna 有一个 baseline；\nlerna add：将本地或远程的包作为依赖添加至当前的 monorepo 仓库中，该命令让 Lerna 可以识别并追踪包之间的依赖关系，因此非常重要；\n\n参考All in one：项目级 monorepo 策略最佳实践Monorepo 最佳实践之 Yarn Workspaces现代前端工程为什么越来越离不开 Monorepo?lerna 多包管理实践\n","slug":"monorepo","date":"2022-02-09T23:18:46.000Z","categories_index":"","tags_index":"Monorepo","author_index":"Matrix"},{"id":"51e9acccf01c1d764ee3a75bbf79b49b","title":"HTTP2","content":"维基百科关于 HTTP&#x2F;2 的介绍，可以看下定义和发展历史:\nWiki\nRFC 7540 定义了 HTTP&#x2F;2 的协议规范和细节，本文的细节主要来自此文档，建议先看一遍本文，再回过头来照着协议大致过一遍 RFC，如果想深入某些细节再仔细翻看 RFC\nRFC7540\nWhy use it ?HTTP&#x2F;1.1 存在的问题:\n\nTCP 连接数限制对于同一个域名，浏览器最多只能同时创建 6 - 8 个 TCP 连接 (不同浏览器不一样)。为了解决数量限制，出现了 域名分片 技术，其实就是资源分域，将资源放在不同域名下 (比如二级子域名下)，这样就可以针对不同域名创建连接并请求，以一种讨巧的方式突破限制，但是滥用此技术也会造成很多问题，比如每个 TCP 连接本身需要经过 DNS 查询、三步握手、慢启动等，还占用额外的 CPU 和内存，对于服务器来说过多连接也容易造成网络拥挤、交通阻塞等，对于移动端来说问题更明显，可以参考这篇文章: Why Domain Sharding is Bad News for Mobile Performance and Users。在图中可以看到新建了六个 TCP 连接，每次新建连接 DNS 解析需要时间(几 ms 到几百 ms 不等)、TCP 慢启动也需要时间、TLS 握手又要时间，而且后续请求都要等待队列调度\n线头阻塞 (Head Of Line Blocking) 问题每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。为了解决此问题，出现了 管线化 - pipelining 技术，但是管线化存在诸多问题，比如第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化\nHeader 内容多，而且每次请求 Header 不会变化太多，没有相应的压缩传输优化方案\n为了尽可能减少请求数，需要做合并文件、雪碧图、资源内联等优化工作，但是这无疑造成了单个请求内容变大延迟变高的问题，且内嵌的资源不能有效地使用缓存机制\n明文传输不安全\n\nHTTP2 的优势二进制分帧层 (Binary Framing Layer)帧是数据传输的最小单位，以二进制传输代替原本的明文传输，原本的报文消息被划分为更小的数据帧:\n\nh1 和 h2 的报文对比:\n\n图中 h2 的报文是重组解析过后的，可以发现一些头字段发生了变化，而且所有头字段均小写\n多路复用 (MultiPlexing)在一个 TCP 连接上，我们可以向对方不断发送帧，每帧的 stream identifier 的标明这一帧属于哪个流，然后在对方接收时，根据 stream identifier 拼接每个流的所有帧组成一整块数据。把 HTTP&#x2F;1.1 每个请求都当作一个流，那么多个请求变成多个流，请求响应数据分成多个帧，不同流中的帧交错地发送给对方，这就是 HTTP&#x2F;2 中的多路复用。\n流的概念实现了单连接上多请求 - 响应并行，解决了线头阻塞的问题，减少了 TCP 连接数量和 TCP 连接慢启动造成的问题\n所以 http2 对于同一域名只需要创建一个连接，而不是像 http&#x2F;1.1 那样创建 6~8 个连接:\n\n服务端推送 (Server Push)浏览器发送一个请求，服务器主动向浏览器推送与这个请求相关的资源，这样浏览器就不用发起后续请求。Server-Push 主要是针对资源内联做出的优化，相较于 http&#x2F;1.1 资源内联的优势:\n\n客户端可以缓存推送的资源\n客户端可以拒收推送过来的资源\n推送资源可以由不同页面共享\n服务器可以按照优先级推送资源\n\nHeader 压缩 (HPACK)使用 HPACK 算法来压缩首部内容上图来自 Ilya Grigorik 的 PPT - HTTP&#x2F;2 is here, let’s optimize!\n可以清楚地看到 HTTP2 头部使用的也是键值对形式的值，而且 HTTP1 当中的请求行以及状态行也被分割成键值对，还有所有键都是小写，不同于 HTTP1。除此之外，还有一个包含静态索引表和动态索引表的索引空间，实际传输时会把头部键值表压缩，使用的算法即 HPACK，其原理就是匹配当前连接存在的索引空间，若某个键值已存在，则用相应的索引代替首部条目，比如 “:method: GET” 可以匹配到静态索引中的 index 2，传输时只需要传输一个包含 2 的字节即可；若索引空间中不存在，则用字符编码传输，字符编码可以选择哈夫曼编码，然后分情况判断是否需要存入动态索引表中\n\n静态索引静态索引表是固定的，对于客户端服务端都一样，目前协议商定的静态索引包含 61 个键值，详见 Static Table Definition - RFC 7541，比如前几个如下\n\n\n索引\n字段值\n键值\n\n\n\nindex\nHeader Name\nHeader Value\n\n\n1\n:authority\n单元格\n\n\n2\n:method\nGET\n\n\n3\n:method\nPOST\n\n\n4\n:path\n&#x2F;\n\n\n5\n:path\n&#x2F;index.html\n\n\n6\n:scheme\nhttp\n\n\n7\n:scheme\nhttps\n\n\n8\n:status\n200\n\n\n\n动态索引动态索引表是一个 FIFO 队列维护的有空间限制的表，里面含有非静态表的索引。动态索引表是需要连接双方维护的，其内容基于连接上下文，一个 HTTP2 连接有且仅有一份动态表。当一个首部匹配不到索引时，可以选择把它插入动态索引表中，下次同名的值就可能会在表中查到索引并替换。但是并非所有首部键值都会存入动态索引，因为动态索引表是有空间限制的，最大值由 SETTING 帧中的 SETTINGS_HEADER_TABLE_SIZE (默认 4096 字节) 设置\n\n应用层的重置连接对于 HTTP&#x2F;1 来说，是通过设置 tcp segment 里的 reset flag 来通知对端关闭连接的。这种方式会直接断开连接，下次再发请求就必须重新建立连接。HTTP&#x2F;2 引入 RST_STREAM 类型的 frame，可以在不断开连接的前提下取消某个 request 的 stream，表现更好。\n请求优先级设置HTTP&#x2F;2 里的每个 stream 都可以设置依赖 (Dependency) 和权重，可以按依赖树分配优先级，解决了关键请求被阻塞的问题\nHTTP&#x2F;1 的几种优化可以弃用合并文件、内联资源、雪碧图、域名分片对于 HTTP&#x2F;2 来说是不必要的，使用 h2 尽可能将资源细粒化，文件分解地尽可能散，不用担心请求数多\n参考HTTP2 详解HTTP1.0、HTTP1.1 和 HTTP2.0 的区别\n","slug":"http2","date":"2022-02-08T22:19:20.000Z","categories_index":"","tags_index":"Network","author_index":"Matrix"},{"id":"acb839066d366821abd556a15e4a7b04","title":"CDN","content":"什么是 CDN？CDN 的全称是 Content Delivery Network，即内容分发网络。是指一种通过互联网互相连接的电脑网络系统，利用最靠近每位用户的服务器，更快、更可靠地将音乐、图片、视频、应用程序及其他文件发送给用户，来提供高性能、可扩展性及低成本的网络内容传递给用户。\n现实中我们都用过天猫超市，在上面买东西非常方便。天猫超市的模式是货品先入天猫超市（后文简称为”猫超”）的菜鸟仓，然后由猫超统一派送的。\n\nCDN 的基本工作过程一图秒懂 CDN 原理（以访问网站某网页资源请求为例）\n引入 CDN 之前\n用户在自己的浏览器中输入要访问的网站域名\n浏览器向本地 DNS 服务器请求对该域名的解析\n本地 DNS 服务器中如果缓存有这个域名的解析结果，则直接响应用户的解析请求\n本地 DNS 服务器中如果没有关于这个域名的解析结果的缓存，则以迭代方式向整个 DNS 系统请求解析，获得应答后将结果反馈给浏览器\n浏览器得到域名解析结果，就是该域名相应的服务设备的 IP 地址\n浏览器获取 IP 地址之后，经过标准的 TCP 握手流程，建立 TCP 连接\n浏览器向服务器发起 HTTP 请求\n服务器将用户请求内容传送给浏览器\n经过标准的 TCP 挥手流程，断开 TCP 连接\n\n引入 CDN 之后\n当用户点击网站页面上的内容 URL，先经过本地 DNS 系统解析，如果本地 DNS 服务器没有相应域名的缓存，则本地 DNS 系统会将域名的解析权交给 CNAME 指向的 CDN 专用 DNS 服务器\nCDN 的 DNS 服务器将 CDN 的全局负载均衡设备 IP 地址返回给用户\n用户向 CDN 的全局负载均衡设备发起 URL 访问请求\nCDN 全局负载均衡设备根据用户 IP 地址，以及用户请求的 URL，选择一台用户所属区域的区域负载均衡设备，并将请求转发到此设备上\n基于以下这些条件的综合分析之后，区域负载均衡设备会选择一个最优的缓存服务器节点，并从缓存服务器节点处得到缓存服务器的 IP 地址，最终将得到的 IP 地址返回给全局负载均衡设备\n根据用户 IP 地址，判断哪一个边缘节点距用户最近\n根据用户所请求的 URL 中携带的内容名称，判断哪一个边缘节点上有用户所需内容\n查询各个边缘节点当前的负载情况，判断哪一个边缘节点尚有服务能力\n\n\n全局负载均衡设备把服务器的 IP 地址返回给用户\n用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地\n\nCNAME 记录是什么？他存在的意义是什么？记录就是把一个域名解析到一个 IP 地址（Address，特制数字 IP 地址），而 CNAME 记录就是把域名解析到另外一个域名。其功能是差不多，CNAME 将几个主机名指向一个别名，其实跟指向 IP 地址是一样的，因为这个别名也要做一个 A 记录的。但是使用 CNAME 记录可以很方便地变更 IP 地址。如果一台服务器有 100 个网站，他们都做了别名，该台服务器变更 IP 时，只需要变更别名的 A 记录就可以了。\n如果服务商给你一个 ip，假如哪天服务商想把 ip 地址换一个，很多人域名上对应的 ip 地址就要跟着变化，要让所有人都一起改完，完全没有办法做到的事情，换成 cname 就没事了，你用你的 cdn，他改他的 ip 地址。唯一的坏处就是，第一次 DNS 解析域名的时候会多解析一次。总结来看，好处远大于坏处。\n什么是 CNAME 记录？CName 记录是 Canonical Name 的简称，通常称别名指向，CNAME 记录可用于将一个域名别名为另一个规范名称的域名系统（DNS）资源记录。　　网站是由一组由一组唯一标识的位置（称为 IP 地址）提供服务的；但是要访问这些站点（例如：晓得博客），我们通常会键入它们对应的域名，这些域名更容易记住。要找到正确的 IP 地址，您的浏览器将联系域名服务器（DNS），并在其数据库中查询 IP 地址。\nCNAME 记录如何使用？例如，假设您有几个子域，例如http://www.mydomain.com，http://ftp.mydomain.com，http://mail.mydomain.com等，并且您希望这些子域指向您的主域名http://mydomain.com。您可以创建CNAME记录，而不是为每个子域创建A记录并将其绑定到您域的IP地址。如下表所示，如果服务器的 IP 地址发生更改，则只需更新一个 A 记录，并且所有子域都会自动更新，因为所有 CNAMES 都指向带有 A 记录的主域：http://mydomain.com指向服务器IP地址，并通过http://www.mydomain.com指向相同的地址http://mydomain.com。如果IP地址发生更改，则只需要在一个地方进行更新即可：只需为修改A记录http://mydomain.com，那么http://www.mydomain.com自动继承更改。　　 CNAME 记录必须始终指向另一个域名，永远不要直接指向 IP 地址。如果您尝试将 CNAME 记录指向 IP 地址，DNSimple 的记录编辑器会警告您。CNAME 对其他记录必须是唯一的。CNAME 记录局限性　　 CNAME 记录必须始终指向另一个域名，并且永远不要直接指向 IP 地址。　　您不能为主域名（http://mydomain.com）本身创建CNAME记录，该记录必须是A记录。　　例如，您不能将http://mydomain.com映射到http://google.com，但是可以将http://google.mydomain.com映射到http://google.com。　　使用 CNAME 记录意味着有一个额外的请求发送到 DNS 服务器，这可能会导致几毫秒的延迟。　　一个 CNAME 记录不能与另一个具有相同名称的记录共存。不能同时有 CNAME 和 TXT 记录http://www.example.com。　　一个 CNAME 可以指向另一个 CNAME，尽管出于性能原因通常不建议使用此配置。如果适用，CNAME 应该尽可能地指向目标名称，以避免不必要的性能开销。\n如何解决 CDN 缓存\n资源 url 参数加时间戳url 的参数加上时间戳，每次更新时时间戳也跟随更新，重新使 cdn 边缘节点同步源服务器最新数据。调用 cdn 服务商提供的刷新缓存接口\n\nhttp://www.cdn.com/static/images/test.png # 没加时间戳\nhttp://www.cdn.com/static/images/test.png?_t=202012290910 # 加了时间戳\n\n\n调用 cdn 服务商提供的刷新缓存接口CDN 边缘节点对开发者是透明的，相比于浏览器 Ctrl+F5 的强制刷新来使浏览器本地缓存失效，开发者可以通过 CDN 服务商提供的“刷新缓存”接口来达到清理 CDN 边缘节点缓存的目的。这样开发者在更新数据后，可以使用“刷新缓存”功能来强制 CDN 节点上的数据缓存过期，保证客户端在访问时，拉取到最新的数据。\n\nCDN 的应用场景\n网站站点&#x2F;应用加速站点或者应用中大量静态资源的加速分发，建议将站点内容进行动静分离，动态文件可以结合云服务器 ECS，静态资源如各类型图片、html、css、js 文件等，建议结合 对象存储 OSS 存储海量静态资源，可以有效加速内容加载速度，轻松搞定网站图片、短视频等内容分发\n视音频点播&#x2F;大文件下载分发加速支持各类文件的下载、分发，支持在线点播加速业务，如 mp4、flv 视频文件或者平均单个文件大小在 20M 以上，主要的业务场景是视音频点播、大文件下载（如安装包下载）等，建议搭配对象存储 OSS 使用，可提升回源速度，节约近 2&#x2F;3 回源带宽成本。\n视频直播加速（内测中）视频流媒体直播服务，支持媒资存储、切片转码、访问鉴权、内容分发加速一体化解决方案。结合弹性伸缩服务，及时调整服务器带宽，应对突发访问流量；结合媒体转码服务，享受高速稳定的并行转码，且任务规模无缝扩展。目前 CDN 直播加速已服务内部用户测试并优化，即将上线\n移动应用加速移动 APP 更新文件（apk 文件）分发，移动 APP 内图片、页面、短视频、UGC 等内容的优化加速分发。提供 httpDNS 服务，避免 DNS 劫持并获得实时精确的 DNS 解析结果，有效缩短用户访问时间，提升用户体验。\n\n参考也许是史上最全的一次 CDN 详解一图秒懂 CDN 原理\n","slug":"cdn","date":"2022-02-08T19:56:46.000Z","categories_index":"","tags_index":"Network","author_index":"Matrix"}]